import os
import csv
import json
import time
import logging
import requests
import warnings
from queue import Queue
from datetime import datetime
from urllib.parse import urlparse
import concurrent.futures

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="ğŸ˜ %(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)

warnings.filterwarnings("ignore", message="Unverified HTTPS request is being made.*")

# è¯·æ±‚å¤´ç»Ÿä¸€é…ç½®
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36 "
        "(check-flink/2.0; +https://github.com/willow-god/check-flink)"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "X-Check-Flink": "1.0"
}

RAW_HEADERS = {  # ä»…ç”¨äºè·å–åŸå§‹æ•°æ®ï¼Œé˜²æ­¢æ¥æ”¶åˆ°Accept-Languageç­‰å¤´éƒ¨å¯¼è‡´ä¹±ç 
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36 "
        "(check-flink/2.0; +https://github.com/willow-god/check-flink)"
    ),
    "X-Check-Flink": "2.0"
}

PROXY_URL_TEMPLATE = f"{os.getenv('PROXY_URL')}{{}}" if os.getenv("PROXY_URL") else None
SOURCE_URL = os.getenv("SOURCE_URL", "https://blog.liushen.fun/flink_count.json")  # é»˜è®¤æœ¬åœ°æ–‡ä»¶
RESULT_FILE = "./result.json"
AUTHOR_URL = os.getenv("AUTHOR_URL", "blog.liushen.fun")  # ä½œè€…URLï¼Œç”¨äºæ£€æµ‹åé“¾
api_request_queue = Queue()

if PROXY_URL_TEMPLATE:
    logging.info("ä»£ç† URL è·å–æˆåŠŸï¼Œä»£ç†åè®®: %s", PROXY_URL_TEMPLATE.split(":")[0])
else:
    logging.info("æœªæä¾›ä»£ç† URL")

if AUTHOR_URL:
    logging.info("ä½œè€… URL: %s", AUTHOR_URL)
else:
    logging.warning("æœªæä¾›ä½œè€… URLï¼Œå°†è·³è¿‡å‹é“¾é¡µé¢æ£€æµ‹")

def request_url(session, url, headers=HEADERS, desc="", timeout=15, verify=True, **kwargs):
    """ç»Ÿä¸€å°è£…çš„ GET è¯·æ±‚å‡½æ•°"""
    try:
        start_time = time.time()
        response = session.get(url, headers=headers, timeout=timeout, verify=verify, **kwargs)
        latency = round(time.time() - start_time, 2)
        return response, latency
    except requests.RequestException as e:
        logging.warning(f"[{desc}] è¯·æ±‚å¤±è´¥: {url}ï¼Œé”™è¯¯å¦‚ä¸‹: \n================================================================\n{e}\n================================================================")
        return None, -1

def load_previous_results():
    if os.path.exists(RESULT_FILE):
        try:
            with open(RESULT_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            logging.warning("JSON è§£æé”™è¯¯ï¼Œä½¿ç”¨ç©ºæ•°æ®")
    return {}

def save_results(data):
    with open(RESULT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def is_url(path):
    return urlparse(path).scheme in ("http", "https")

def check_author_link_in_page(session, linkpage_url):
    """æ£€æµ‹å‹é“¾é¡µé¢æ˜¯å¦åŒ…å«ä½œè€…é“¾æ¥"""
    if not AUTHOR_URL:
        return False
    
    response, _ = request_url(session, linkpage_url, headers=RAW_HEADERS, desc="å‹é“¾é¡µé¢æ£€æµ‹")
    if not response:
        return False
    
    # å¤„ç†ä½œè€…URLï¼Œç¡®ä¿æœ‰åè®®å·
    author_url = AUTHOR_URL
    if not author_url.startswith(('http://', 'https://')):
        author_url = 'https://' + author_url
    
    # ç”Ÿæˆå„ç§å¯èƒ½çš„URLå˜ä½“
    author_variants = [
        author_url,
        author_url.replace('https://', 'http://'),
        author_url.replace('https://', '//'),
        author_url.replace('https://', ''),
        AUTHOR_URL,  # åŸå§‹å€¼ï¼ˆå¯èƒ½æ²¡æœ‰åè®®å·ï¼‰
        '//' + AUTHOR_URL,
        'https://' + AUTHOR_URL,
        'http://' + AUTHOR_URL
    ]
    
    # å»é‡
    author_variants = list(set(author_variants))
    
    content = response.text
    found_in_href = False
    found_as_text = False
    
    # æ£€æŸ¥æ¯ç§å˜ä½“
    for variant in author_variants:
        # æ£€æŸ¥æ˜¯å¦åœ¨hrefå±æ€§ä¸­
        if f'href="{variant}"' in content or \
           f"href='{variant}'" in content or \
           f'href="{variant}/"' in content or \
           f"href='{variant}/'" in content:
            found_in_href = True
            break
        
        # æ£€æŸ¥æ˜¯å¦ä½œä¸ºæ–‡æœ¬å‡ºç°
        if variant in content:
            found_as_text = True
    
    if found_in_href:
        logging.info(f"å‹é“¾é¡µé¢ {linkpage_url} ä¸­æ‰¾åˆ°ä½œè€…é“¾æ¥: {author_url}")
        return True
    elif found_as_text:
        logging.info(f"å‹é“¾é¡µé¢ {linkpage_url} ä¸­åŒ…å«ä½œè€…URLæ–‡æœ¬ä½†éé“¾æ¥")
        return True
    else:
        logging.info(f"å‹é“¾é¡µé¢ {linkpage_url} ä¸­æœªæ‰¾åˆ°ä½œè€…é“¾æ¥")
        return False

def fetch_origin_data(origin_path):
    logging.info(f"æ­£åœ¨è¯»å–æ•°æ®æº: {origin_path}")
    try:
        if is_url(origin_path):
            with requests.Session() as session:
                response, _ = request_url(session, origin_path, headers=RAW_HEADERS, desc="æ•°æ®æº")
                content = response.text if response else ""
        else:
            with open(origin_path, "r", encoding="utf-8") as f:
                content = f.read()
    except Exception as e:
        logging.error(f"è¯»å–æ•°æ®å¤±è´¥: {e}")
        return []

    try:
        data = json.loads(content)
        if isinstance(data, dict) and 'link_list' in data:
            logging.info("æˆåŠŸè§£æ JSON æ ¼å¼æ•°æ®")
            return data['link_list']
        elif isinstance(data, list):
            logging.info("æˆåŠŸè§£æ JSON æ•°ç»„æ ¼å¼æ•°æ®")
            return data
    except json.JSONDecodeError:
        pass

    try:
        rows = list(csv.reader(content.splitlines()))
        logging.info("æˆåŠŸè§£æ CSV æ ¼å¼æ•°æ®")
        # æ”¯æŒæ–°çš„CSVæ ¼å¼ï¼šname, link, linkpage
        result = []
        for row in rows:
            if len(row) >= 2:
                item = {'name': row[0], 'link': row[1]}
                if len(row) >= 3 and row[2].strip():
                    item['linkpage'] = row[2].strip()
                result.append(item)
        return result
    except Exception as e:
        logging.error(f"CSV è§£æå¤±è´¥: {e}")
        return []

def check_link(item, session):
    link = item['link']
    has_author_link = False
    
    for method, url in [("ç›´æ¥è®¿é—®", link), ("ä»£ç†è®¿é—®", PROXY_URL_TEMPLATE.format(link) if PROXY_URL_TEMPLATE else None)]:
        if not url or not is_url(url):
            logging.warning(f"[{method}] æ— æ•ˆé“¾æ¥: {link}")
            continue
        response, latency = request_url(session, url, desc=method)
        if response and response.status_code == 200:
            logging.info(f"[{method}] æˆåŠŸè®¿é—®: {link} ï¼Œå»¶è¿Ÿ {latency} ç§’")
            
            # å¦‚æœé“¾æ¥å¯è¾¾ä¸”æœ‰linkpageå­—æ®µï¼Œæ£€æµ‹å‹é“¾é¡µé¢
            if 'linkpage' in item and item['linkpage'] and AUTHOR_URL:
                has_author_link = check_author_link_in_page(session, item['linkpage'])
            
            return item, latency, has_author_link
        elif response and response.status_code != 200:
            logging.warning(f"[{method}] çŠ¶æ€ç å¼‚å¸¸: {link} -> {response.status_code}")
        else:
            logging.warning(f"[{method}] è¯·æ±‚å¤±è´¥ï¼ŒResponse æ— æ•ˆ: {link}")

    api_request_queue.put(item)
    return item, -1, False

def handle_api_requests(session):
    results = []
    while not api_request_queue.empty():
        time.sleep(0.2)
        item = api_request_queue.get()
        link = item['link']
        api_url = f"https://v2.xxapi.cn/api/status?url={link}"
        response, latency = request_url(session, api_url, headers=RAW_HEADERS, desc="API æ£€æŸ¥", timeout=30)
        has_author_link = False
        
        if response:
            try:
                res_json = response.json()
                if int(res_json.get("code")) == 200 and int(res_json.get("data")) == 200:
                    logging.info(f"[API] æˆåŠŸè®¿é—®: {link} ï¼ŒçŠ¶æ€ç  200")
                    item['latency'] = latency
                    
                    # å¦‚æœAPIæ£€æµ‹æˆåŠŸä¸”æœ‰linkpageå­—æ®µï¼Œæ£€æµ‹å‹é“¾é¡µé¢
                    if 'linkpage' in item and item['linkpage'] and AUTHOR_URL:
                        has_author_link = check_author_link_in_page(session, item['linkpage'])
                else:
                    logging.warning(f"[API] çŠ¶æ€å¼‚å¸¸: {link} -> [{res_json.get('code')}, {res_json.get('data')}]")
                    item['latency'] = -1
            except Exception as e:
                logging.error(f"[API] è§£æå“åº”å¤±è´¥: {link}ï¼Œé”™è¯¯: {e}")
                item['latency'] = -1
        else:
            item['latency'] = -1
        
        results.append((item, item.get('latency', -1), has_author_link))
    return results

def main():
    try:
        link_list = fetch_origin_data(SOURCE_URL)
        if not link_list:
            logging.error("æ•°æ®æºä¸ºç©ºæˆ–è§£æå¤±è´¥")
            return

        previous_results = load_previous_results()

        with requests.Session() as session:
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                results = list(executor.map(lambda item: check_link(item, session), link_list))

            updated_api_results = handle_api_requests(session)
            for updated_item in updated_api_results:
                for idx, (item, latency, has_author) in enumerate(results):
                    if item['link'] == updated_item[0]['link']:
                        results[idx] = updated_item
                        break

        current_links = {item['link'] for item in link_list}
        link_status = []

        for item, latency, has_author_link in results:
            try:
                name = item.get('name', 'æœªçŸ¥')
                link = item.get('link')
                if not link:
                    logging.warning(f"è·³è¿‡æ— æ•ˆé¡¹: {item}")
                    continue

                prev_entry = next((x for x in previous_results.get("link_status", []) if x.get("link") == link), {})
                prev_fail_count = prev_entry.get("fail_count", 0)
                fail_count = prev_fail_count + 1 if latency == -1 else 0

                link_status.append({
                    'name': name,
                    'link': link,
                    'latency': latency,
                    'fail_count': fail_count,
                    'has_author_link': has_author_link,  # æ–°å¢å­—æ®µ
                    'linkpage': item.get('linkpage', '')  # ä¿ç•™linkpageä¿¡æ¯
                })
            except Exception as e:
                logging.error(f"å¤„ç†é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯: {item}, é”™è¯¯: {e}")

        link_status = [entry for entry in link_status if entry["link"] in current_links]

        accessible = sum(1 for x in link_status if x["latency"] != -1)
        has_author_count = sum(1 for x in link_status if x["has_author_link"])
        total = len(link_status)
        output = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "accessible_count": accessible,
            "inaccessible_count": total - accessible,
            "total_count": total,
            "has_author_link_count": has_author_count,  # æ–°å¢ç»Ÿè®¡
            "author_url": AUTHOR_URL,  # è®°å½•ä½¿ç”¨çš„ä½œè€…URL
            "link_status": link_status
        }

        save_results(output)
        logging.info(f"å…±æ£€æŸ¥ {total} ä¸ªé“¾æ¥ï¼ŒæˆåŠŸ {accessible} ä¸ªï¼Œå¤±è´¥ {total - accessible} ä¸ª")
        logging.info(f"å…¶ä¸­ {has_author_count} ä¸ªå‹é“¾é¡µé¢åŒ…å«ä½œè€…é“¾æ¥")
        logging.info(f"ç»“æœå·²ä¿å­˜è‡³: {RESULT_FILE}")
    except Exception as e:
        logging.exception(f"è¿è¡Œä¸»ç¨‹åºå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
