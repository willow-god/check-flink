import os
import csv
import json
import time
import logging
import requests
import warnings
from queue import Queue
from datetime import datetime
from urllib.parse import urlparse
import concurrent.futures

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="ğŸ˜ %(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)

warnings.filterwarnings("ignore", message="Unverified HTTPS request is being made.*")

# è¯·æ±‚å¤´ç»Ÿä¸€é…ç½®
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36 "
        "(check-flink/1.0; +https://github.com/willow-god/check-flink)"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "X-Check-Flink": "1.0"
}

RAW_HEADERS = {  # ä»…ç”¨äºè·å–åŸå§‹æ•°æ®ï¼Œé˜²æ­¢æ¥æ”¶åˆ°Accept-Languageç­‰å¤´éƒ¨å¯¼è‡´ä¹±ç 
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36 "
        "(check-flink/1.0; +https://github.com/willow-god/check-flink)"
    ),
    "X-Check-Flink": "1.0"
}

PROXY_URL_TEMPLATE = f"{os.getenv('PROXY_URL')}{{}}" if os.getenv("PROXY_URL") else None
SOURCE_URL = os.getenv("SOURCE_URL", "./link.csv")  # é»˜è®¤æœ¬åœ°æ–‡ä»¶
RESULT_FILE = "./result.json"
api_request_queue = Queue()

if PROXY_URL_TEMPLATE:
    logging.info("ä»£ç† URL è·å–æˆåŠŸï¼Œä»£ç†åè®®: %s", PROXY_URL_TEMPLATE.split(":")[0])

else:
    logging.info("æœªæä¾›ä»£ç† URL")

def request_url(session, url, headers=HEADERS, desc="", timeout=15, verify=True, **kwargs):
    """ç»Ÿä¸€å°è£…çš„ GET è¯·æ±‚å‡½æ•°"""
    try:
        start_time = time.time()
        response = session.get(url, headers=headers, timeout=timeout, verify=verify, **kwargs)
        latency = round(time.time() - start_time, 2)
        return response, latency
    except requests.RequestException as e:
        logging.warning(f"[{desc}] è¯·æ±‚å¤±è´¥: {url}ï¼Œé”™è¯¯å¦‚ä¸‹: \n================================================================\n{e}\n================================================================")
        return None, -1

def load_previous_results():
    if os.path.exists(RESULT_FILE):
        try:
            with open(RESULT_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            logging.warning("JSON è§£æé”™è¯¯ï¼Œä½¿ç”¨ç©ºæ•°æ®")
    return {}

def save_results(data):
    with open(RESULT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def is_url(path):
    return urlparse(path).scheme in ("http", "https")

def fetch_origin_data(origin_path):
    logging.info(f"æ­£åœ¨è¯»å–æ•°æ®æº: {origin_path}")
    try:
        if is_url(origin_path):
            with requests.Session() as session:
                response, _ = request_url(session, origin_path, headers=RAW_HEADERS, desc="æ•°æ®æº")
                content = response.text if response else ""
        else:
            with open(origin_path, "r", encoding="utf-8") as f:
                content = f.read()
    except Exception as e:
        logging.error(f"è¯»å–æ•°æ®å¤±è´¥: {e}")
        return []

    try:
        data = json.loads(content)
        if isinstance(data, dict) and 'link_list' in data:
            logging.info("æˆåŠŸè§£æ JSON æ ¼å¼æ•°æ®")
            return data['link_list']
        elif isinstance(data, list):
            logging.info("æˆåŠŸè§£æ JSON æ•°ç»„æ ¼å¼æ•°æ®")
            return data
    except json.JSONDecodeError:
        pass

    try:
        rows = list(csv.reader(content.splitlines()))
        logging.info("æˆåŠŸè§£æ CSV æ ¼å¼æ•°æ®")
        return [{'name': row[0], 'link': row[1]} for row in rows if len(row) == 2]
    except Exception as e:
        logging.error(f"CSV è§£æå¤±è´¥: {e}")
        return []

def check_link(item, session):
    link = item['link']
    for method, url in [("ç›´æ¥è®¿é—®", link), ("ä»£ç†è®¿é—®", PROXY_URL_TEMPLATE.format(link) if PROXY_URL_TEMPLATE else None)]:
        if not url or not is_url(url):
            logging.warning(f"[{method}] æ— æ•ˆé“¾æ¥: {link}")
            continue
        response, latency = request_url(session, url, desc=method)
        if response and response.status_code == 200:
            logging.info(f"[{method}] æˆåŠŸè®¿é—®: {link} ï¼Œå»¶è¿Ÿ {latency} ç§’")
            return item, latency
        elif response and response.status_code != 200:
            logging.warning(f"[{method}] çŠ¶æ€ç å¼‚å¸¸: {link} -> {response.status_code}")
        else:
            logging.warning(f"[{method}] è¯·æ±‚å¤±è´¥ï¼ŒResponse æ— æ•ˆ: {link}")

    api_request_queue.put(item)
    return item, -1

def handle_api_requests(session):
    results = []
    while not api_request_queue.empty():
        time.sleep(0.2)
        item = api_request_queue.get()
        link = item['link']
        api_url = f"https://v2.xxapi.cn/api/status?url={link}"
        response, latency = request_url(session, api_url,headers=RAW_HEADERS, desc="API æ£€æŸ¥", timeout=30)
        if response:
            try:
                res_json = response.json()
                if int(res_json.get("code")) == 200 and int(res_json.get("data")) == 200:
                    logging.info(f"[API] æˆåŠŸè®¿é—®: {link} ï¼ŒçŠ¶æ€ç  200")
                    item['latency'] = latency
                else:
                    logging.warning(f"[API] çŠ¶æ€å¼‚å¸¸: {link} -> [{res_json.get('code')}, {res_json.get('data')}]")
                    item['latency'] = -1
            except Exception as e:
                logging.error(f"[API] è§£æå“åº”å¤±è´¥: {link}ï¼Œé”™è¯¯: {e}")
                item['latency'] = -1
        else:
            item['latency'] = -1
        results.append(item)
    return results

def main():
    try:
        link_list = fetch_origin_data(SOURCE_URL)
        if not link_list:
            logging.error("æ•°æ®æºä¸ºç©ºæˆ–è§£æå¤±è´¥")
            return

        previous_results = load_previous_results()

        with requests.Session() as session:
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                results = list(executor.map(lambda item: check_link(item, session), link_list))

            updated_api_results = handle_api_requests(session)
            for updated_item in updated_api_results:
                for idx, (item, latency) in enumerate(results):
                    if item['link'] == updated_item['link']:
                        results[idx] = (item, updated_item['latency'])
                        break

        current_links = {item['link'] for item in link_list}
        link_status = []

        for item, latency in results:
            try:
                name = item.get('name', 'æœªçŸ¥')
                link = item.get('link')
                if not link:
                    logging.warning(f"è·³è¿‡æ— æ•ˆé¡¹: {item}")
                    continue

                prev_entry = next((x for x in previous_results.get("link_status", []) if x.get("link") == link), {})
                prev_fail_count = prev_entry.get("fail_count", 0)
                fail_count = prev_fail_count + 1 if latency == -1 else 0

                link_status.append({
                    'name': name,
                    'link': link,
                    'latency': latency,
                    'fail_count': fail_count
                })
            except Exception as e:
                logging.error(f"å¤„ç†é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯: {item}, é”™è¯¯: {e}")

        link_status = [entry for entry in link_status if entry["link"] in current_links]

        accessible = sum(1 for x in link_status if x["latency"] != -1)
        total = len(link_status)
        output = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "accessible_count": accessible,
            "inaccessible_count": total - accessible,
            "total_count": total,
            "link_status": link_status
        }

        save_results(output)
        logging.info(f"å…±æ£€æŸ¥ {total} ä¸ªé“¾æ¥ï¼ŒæˆåŠŸ {accessible} ä¸ªï¼Œå¤±è´¥ {total - accessible} ä¸ª")
        logging.info(f"ç»“æœå·²ä¿å­˜è‡³: {RESULT_FILE}")
    except Exception as e:
        logging.exception(f"è¿è¡Œä¸»ç¨‹åºå¤±è´¥: {e}")

if __name__ == "__main__":
    main()